# AI-based Transaction Classification
# Tri-Tower Model Architecture

---

## Input Layer

The model takes in three types of input features:

### 1. **Context Tower Input (Transaction Features)**
Numerical and categorical attributes extracted directly from each transaction:

- **Text Embeddings (384 dimensions)**: 
  - Vietnamese text from transaction messages (`msg_content`) embedded using pretrained FastText model (`cc.vi.300.bin`)
  - Captures semantic meaning of merchant names, user memos, and payment descriptions
  
- **Numerical Features (3 dimensions)**:
  - `amount_log`: Log-transformed transaction amount `log1p(amount)`
  - `hour_sin`, `hour_cos`: Sine and cosine encoding of transaction hour-of-day for cyclical time patterns
  
- **Categorical Features (one-hot encoded)**:
  - `tranx_type`: Transaction type/flow (e.g., TRANSFER, BILL_PAY, QR_PAY, etc.) - ~16 categories
  - `channel`: Transaction channel (MOBILE, WEB) - ~8 categories
  
- **Total Context Input Dimension**: ~411 dimensions (384 text + 3 numerical + ~24 categorical)

### 2. **Sender Tower Input (User Behavioral Features)**
A 128-dimensional vector capturing the sender's historical transaction behavior:

- **Sender Embedding (128 dimensions)**:
  - Learned representation from user's transaction sequence history
  - Generated by a **GRU-based sequence encoder** that processes:
    - Past transaction amounts (log-scaled, standardized)
    - Past transaction types (embedded to 16 dimensions)
    - Past channels (embedded to 8 dimensions)
  - Captures spending patterns, recurring behaviors, channel preferences, and temporal habits
  - Uses **point-in-time features**: only transactions up to `D-1` days before current transaction

### 3. **Recipient Tower Input (Entity Features)**
A 96-dimensional vector representing the recipient entity (merchant or person):

- **Graph-based Collaborative Features (64 dimensions)**:
  - **SVD embeddings** from sender-recipient interaction matrix (adjacency matrix)
  - Built using Singular Value Decomposition on the bipartite graph where:
    - Rows = senders who paid this recipient
    - Columns = recipients
    - Values = log-transformed total amount paid
  - Captures collaborative patterns: "users who pay recipient A also pay recipient B"
  
- **Profile Metadata Features (32 dimensions, hashed)**:
  - For **merchants**: Merchant Category Code (MCC), brand name
  - For **persons**: Age band, province code, occupation band
  - **Feature hashing** to convert categorical metadata into fixed-length vectors
  - Helps with cold-start recipients who lack interaction history

---

## Hidden Layers

The three input towers are processed independently before being fused:

### **Context Encoder (CtxMLP)**
- **Input**: ~411-dimensional concatenated context features
- **Architecture**:
  - Dense layer: 411 → 256 units, ReLU activation
  - LayerNorm (256)
  - Dropout (p=0.2)
  - Dense layer: 256 → 128 units
  - LayerNorm (128)
- **Output**: `h_ctx` - 128-dimensional context embedding

### **Sender Encoder (SndEncoder - GRU)**
- **Input**: Sequence of past transactions (variable length, up to lookback window)
  - Each timestep: 3 numerical + 16 (tranx_type embedding) + 8 (channel embedding) = 27 dims
- **Architecture**:
  - Embedding layers for categorical inputs:
    - `tranx_type` → 16-dim embedding
    - `channel` → 8-dim embedding
  - Dropout (p=0.1) on concatenated inputs
  - **GRU layer**: 27 input dims → 128 hidden units, batch_first=True
  - LayerNorm (128) on final hidden state
- **Output**: `h_snd` - 128-dimensional sender behavioral embedding

### **Recipient Encoder (Graph + Profile Fusion)**
- **Input**: 
  - SVD embeddings (64 dims) from interaction matrix
  - Hashed profile features (32 dims) from metadata
- **Architecture**:
  - Concatenate graph and profile features: 64 + 32 = 96 dims
  - (In current implementation: features are used directly without additional MLP, but can be extended)
- **Output**: `h_rcv` - 96-dimensional recipient embedding

---

## Fusion & Classification Layers

### **Feature Concatenation**
All three tower outputs are concatenated into a single unified representation:
- **Concatenated vector**: `[h_ctx, h_snd, h_rcv]`
- **Total dimension**: 128 + 128 + 96 = **352 dimensions**

### **Multi-Layer Perceptron Classifier (TriTowerClassifier)**
The concatenated embeddings pass through a deep fully-connected classifier:

- **Hidden Layer 1 (512 units)**:
  - Linear: 352 → 512
  - LayerNorm (512)
  - ReLU activation
  - Dropout (p=0.3)
  
- **Hidden Layer 2 (256 units)**:
  - Linear: 512 → 256
  - LayerNorm (256)
  - ReLU activation
  - Dropout (p=0.2)
  
- **Output Layer (10 units, Softmax)**:
  - Linear: 256 → 10 (number of transaction categories)
  - Softmax activation for probability distribution

---

## Output Layer

### **Class Probabilities**
The output is a **10-dimensional probability vector**, indicating the likelihood of the transaction belonging to each predefined category:

| Code | Category Name            | Description                              |
|------|--------------------------|------------------------------------------|
| BIL  | Bills & Utilities        | Electricity, water, internet, rent       |
| FOO  | Food & Drink             | Groceries, restaurants, cafes            |
| TRN  | Transport & Mobility     | Ride-hailing, public transport, fuel     |
| HLT  | Health & Wellness        | Clinics, pharmacies, health services     |
| INS  | Insurance & Taxes        | Insurance premiums, tax payments         |
| SHP  | Shopping & Retail        | E-commerce, retail stores                |
| ENT  | Entertainment & Leisure  | Streaming, games, cinemas, events        |
| EDU  | Education & Learning     | Tuition, courses, educational services   |
| FIN  | Financial Services       | Loans, savings, investments, fees        |
| OTH  | Other                    | Miscellaneous / uncategorized            |

- **Training Loss**: Cross-entropy loss
- **Optimization**: Adam optimizer (learning rate = 1e-3)
- **Metrics**:
  - **Primary metric**: Macro F1 score (treats all categories equally, robust to class imbalance)
  - **Secondary metric**: Macro ROC-AUC (one-vs-rest, measures ranking quality)
  - Accuracy, per-class precision/recall

---

## Architecture Summary Diagram

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                              INPUT LAYER                                         │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  ┌─────────────────────┐  ┌─────────────────────┐  ┌─────────────────────┐   │
│  │  Context Features   │  │  Sender Embedding   │  │ Recipient Embedding │   │
│  │   (Transaction)     │  │   (User History)    │  │  (Entity Profile)   │   │
│  │                     │  │                     │  │                     │   │
│  │ • Text: 384 dims    │  │ • Sequence: var len │  │ • SVD Graph: 64 dims│   │
│  │ • Numeric: 3 dims   │  │ • Embedding: 128 d  │  │ • Metadata: 32 dims │   │
│  │ • Categor: ~24 dims │  │                     │  │                     │   │
│  │ Total: ~411 dims    │  │ Output: 128 dims    │  │ Total: 96 dims      │   │
│  └──────────┬──────────┘  └──────────┬──────────┘  └──────────┬──────────┘   │
│             │                        │                        │              │
└─────────────┼────────────────────────┼────────────────────────┼──────────────┘
              │                        │                        │
              ▼                        ▼                        ▼
┌─────────────────────────────────────────────────────────────────────────────────┐
│                            ENCODER LAYERS                                        │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  ┌─────────────────────┐  ┌─────────────────────┐  ┌─────────────────────┐   │
│  │   CtxMLP Encoder    │  │   GRU Encoder       │  │  Graph + Profile    │   │
│  │                     │  │                     │  │     Fusion          │   │
│  │ Dense: 411 → 256    │  │ Embeddings:         │  │                     │   │
│  │ LayerNorm + ReLU    │  │  - tranx_type: 16   │  │ Concat(SVD, Meta)   │   │
│  │ Dropout(0.2)        │  │  - channel: 8       │  │   96 dimensions     │   │
│  │ Dense: 256 → 128    │  │ GRU: 27 → 128       │  │                     │   │
│  │ LayerNorm           │  │ LayerNorm           │  │ (direct pass-thru)  │   │
│  │                     │  │                     │  │                     │   │
│  │ Output: h_ctx (128) │  │ Output: h_snd (128) │  │ Output: h_rcv (96)  │   │
│  └──────────┬──────────┘  └──────────┬──────────┘  └──────────┬──────────┘   │
│             │                        │                        │              │
└─────────────┼────────────────────────┼────────────────────────┼──────────────┘
              │                        │                        │
              └────────────┬───────────┴────────────┬───────────┘
                           ▼                        ▼
                    ┌──────────────────────────────────────┐
                    │   Concatenate Tower Outputs          │
                    │   [h_ctx, h_snd, h_rcv]              │
                    │   Total: 352 dimensions              │
                    └──────────────┬───────────────────────┘
                                   ▼
┌─────────────────────────────────────────────────────────────────────────────────┐
│                      CLASSIFICATION LAYERS                                       │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│                     Dense Layer 1 (512 units)                                    │
│                     Linear: 352 → 512                                            │
│                     LayerNorm(512) + ReLU                                        │
│                     Dropout(0.3)                                                 │
│                              ▼                                                   │
│                     Dense Layer 2 (256 units)                                    │
│                     Linear: 512 → 256                                            │
│                     LayerNorm(256) + ReLU                                        │
│                     Dropout(0.2)                                                 │
│                              ▼                                                   │
│                     Output Layer (10 units)                                      │
│                     Linear: 256 → 10                                             │
│                     Softmax activation                                           │
│                                                                                  │
└──────────────────────────────────┬───────────────────────────────────────────────┘
                                   ▼
┌─────────────────────────────────────────────────────────────────────────────────┐
│                              OUTPUT LAYER                                        │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│              Class Probabilities: [P(BIL), P(FOO), ..., P(OTH)]                 │
│              10-dimensional probability distribution over categories             │
│                                                                                  │
│              Final prediction: argmax(probabilities)                             │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## Key Design Principles

### **1. Point-in-Time Consistency**
- Sender and recipient features use snapshots from `date = transaction_date - lookback_days`
- Prevents data leakage: the model only sees past behavior, never future information
- Feature store is partitioned by date: `data/feature_store/x_snd/date=YYYY-MM-DD/`

### **2. Temporal Awareness**
- Context encoder uses cyclical time encodings (sine/cosine of hour)
- Sender encoder processes transaction sequences chronologically via GRU
- Training uses temporal train/val split (not random) to mimic production deployment

### **3. Handling Cold-Start Entities**
- **Senders**: Profile features (age_band, segment, income_band) from `customer_profile` provide fallback signals
- **Recipients**: Metadata hashing (MCC, occupation, location) ensures all entities have some representation even with zero transaction history

### **4. Multimodal Fusion**
- **Context tower**: Captures transaction-level signals (what, when, how much)
- **Sender tower**: Captures behavioral patterns (spending habits, preferences)
- **Recipient tower**: Captures entity identity and popularity (who is being paid)
- Late fusion via concatenation allows the classifier to learn optimal weighting

### **5. Regularization & Stability**
- LayerNorm after each dense layer for stable gradients
- Dropout (0.1-0.3) to prevent overfitting
- L2 weight decay (optional, via Adam optimizer)
- Early stopping on validation macro F1 score (patience=3 epochs)

---

## Training Configuration

- **Optimizer**: Adam with learning rate 1e-3
- **Loss function**: Cross-entropy
- **Batch size**: 512
- **Epochs**: 8 (with early stopping, patience=3)
- **Learning rate schedule**: Constant (can be extended with cosine annealing or step decay)
- **Validation strategy**: 
  - Date-based split (last 20% of dates for validation)
  - Alternatively: stratified random split for quick experimentation

---

## Model Checkpoints & Artifacts

All trained components are saved in `models/` directory:

| Artifact                        | File Path                          | Description                                    |
|---------------------------------|------------------------------------|------------------------------------------------|
| Context encoder                 | `ctx_encoder.pt`                   | Pretrained CtxMLP weights                      |
| Context autoencoder             | `ctx_autoencoder.pt`               | Full autoencoder (encoder + decoder)           |
| Sender encoder                  | `snd_encoder.pt`                   | Pretrained SndEncoder (GRU) weights            |
| Tri-tower classifier            | `tri_tower_classifier.pt`          | Final classification model weights             |
| Label encoder                   | `cls_label_map.pkl`                | Label-to-index mapping (10 classes)            |
| Transaction type one-hot encoder| `ctx_tt_ohe.pkl`                   | Scikit-learn OneHotEncoder for tranx_type      |
| Channel one-hot encoder         | `ctx_ch_ohe.pkl`                   | Scikit-learn OneHotEncoder for channel         |
| Numerical scaler                | `ctx_num_scaler.pkl`               | StandardScaler for amount_log and time features|
| Amount quantizer                | `snd_amount_quantiles.pkl`         | Quantile bins for sender sequence preprocessing|

---

## Inference Pipeline (Online Scoring)

For a new transaction `t` at time `τ`:

1. **Extract context features**:
   - Embed `msg_content` using FastText → 384 dims
   - Compute `amount_log`, `hour_sin`, `hour_cos` → 3 dims
   - One-hot encode `tranx_type`, `channel` → ~24 dims
   - Pass through `ctx_encoder.pt` → `h_ctx` (128 dims)

2. **Lookup sender embedding**:
   - Query feature store: `x_snd/date={τ - 1 day}/x_snd_user_state.parquet`
   - Retrieve `snd_emb_*` for `sender_id` → `h_snd` (128 dims)
   - If sender not found, use zero-vector or profile-based fallback

3. **Lookup recipient embedding**:
   - Query feature store: `x_rcv/date={τ - 1 day}/x_rcv_entity_state.parquet`
   - Retrieve `rcv_emb_*` for `recipient_entity_id` → `h_rcv` (96 dims)
   - If recipient not found, use zero-vector or metadata-based fallback

4. **Classify**:
   - Concatenate `[h_ctx, h_snd, h_rcv]` → 352 dims
   - Pass through `tri_tower_classifier.pt` → 10-class probabilities
   - Return predicted category and confidence scores

**Online latency targets**:
- Context encoding: ~5-10 ms (CPU)
- Feature lookup: ~2-5 ms (if cached in Redis/KV store)
- Classification forward pass: ~1-2 ms (CPU)
- **Total**: ~10-20 ms per transaction (suitable for near-real-time scoring)

---

## Future Extensions

1. **Attention mechanism**: Replace simple concatenation with cross-attention between towers
2. **Graph Neural Networks**: Use GNN for recipient tower to model multi-hop graph relationships
3. **Transformer encoders**: Replace GRU with Transformer for sender sequence modeling
4. **Multi-task learning**: Add auxiliary tasks (e.g., predict transaction amount, fraud flag)
5. **Continual learning**: Periodic retraining with incremental data updates, track model drift
6. **Explainability**: Integrate SHAP or attention weights for per-prediction explanations
